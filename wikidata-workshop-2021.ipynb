{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using KGTK query to do interesting queries in Wikidata\n",
    "This notebook shows use cases of interesting queries on Wikidata that can be done using the KGTK query command (aka Kypher), and that cannot be done using the public Wikidata SPARQL endpoint\n",
    "\n",
    "The notebook has a preamble to set up environment variables to access the relevant files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Folder on local machine where to create the output and temporary folders\n",
    "output_path = \"/Users/pedroszekely/Downloads/kypher\"\n",
    "\n",
    "# The names of the output and temporary folders\n",
    "output_folder = \"wd-workshop\"\n",
    "temp_folder = \"temp.wd-workshop\"\n",
    "\n",
    "# The location of input Wikidata files\n",
    "wikidata_folder = \"/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/\"\n",
    "# wikidata_folder = \"/Users/pedroszekely/Downloads/kypher/wikidata_os_v1/\"\n",
    "# The wikidata_os files can be downloaded from https://drive.google.com/drive/folders/1V6oAQKmwQ4LJnrBai-uv5gHWphFSCt50?usp=sharing\n",
    "\n",
    "wikidata_dbpedia_folder = \"/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-dbpedia\"\n",
    "\n",
    "# Location of the cache database for kypher\n",
    "cache_path = \"/Users/pedroszekely/Downloads/kypher/temp.novartis\"\n",
    "# cache_path = \"/Users/pedroszekely/Downloads/kypher/temp.useful_wikidata_files_v4/wikidata.sqlite3.db\"\n",
    "# Whether to delete the cache database\n",
    "delete_database = False\n",
    "\n",
    "# shortcuts to commands\n",
    "kgtk = \"time kgtk --debug\"\n",
    "# kgtk = \"kgtk --debug\"\n",
    "# kgtk = \"kgtk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "import papermill as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = round(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALIAS: \"/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/aliases.en.tsv.gz\"\n",
      "CLAIMS: \"/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/claims.tsv.gz\"\n",
      "DESCRIPTION: \"/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/descriptions.en.tsv.gz\"\n",
      "DWD_ISA: \"/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/derived.dwd_isa.tsv.gz\"\n",
      "EXAMPLES_DIR: \"/Users/pedroszekely/Documents/GitHub/kgtk-at-2021-wikidata-workshop\"\n",
      "EXTERNAL_ID: \"/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/claims.external-id.tsv.gz\"\n",
      "ISA: \"/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/derived.isa.tsv.gz\"\n",
      "ITEM: \"/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/claims.wikibase-item.tsv.gz\"\n",
      "LABEL: \"/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/labels.en.tsv.gz\"\n",
      "OUT: \"/Users/pedroszekely/Downloads/kypher/wd-workshop\"\n",
      "P279: \"/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/derived.P279.tsv.gz\"\n",
      "P279STAR: \"/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/derived.P279star.tsv.gz\"\n",
      "P31: \"/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/derived.P31.tsv.gz\"\n",
      "PROPERTY_DATATYPES: \"/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/metadata.property.datatypes.tsv.gz\"\n",
      "QUALIFIERS: \"/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/qualifiers.tsv.gz\"\n",
      "QUALIFIERS_TIME: \"/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/qualifiers.time.tsv.gz\"\n",
      "QUANTITY: \"/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/claims.quantity.tsv.gz\"\n",
      "SITELINKS: \"/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/sitelinks.tsv.gz\"\n",
      "STORE: \"/Users/pedroszekely/Downloads/kypher/temp.novartis/wikidata.sqlite3.db\"\n",
      "TEMP: \"/Users/pedroszekely/Downloads/kypher/temp.wd-workshop\"\n",
      "TIME: \"/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/claims.time.tsv.gz\"\n",
      "WD2DB: \"/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-dbpedia/wikidata_to_dbpedia_edge_file.tsv.gz\"\n",
      "WIKIDATA: \"/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/\"\n",
      "kgtk: \"time kgtk --debug\"\n",
      "kypher: \"time kgtk --debug query --graph-cache /Users/pedroszekely/Downloads/kypher/temp.novartis/wikidata.sqlite3.db\"\n"
     ]
    }
   ],
   "source": [
    "# The names of files in the KGTK Wikidata distirbution that we will use in this notebook.\n",
    "file_names = {\n",
    "    \"claims\": \"claims.tsv.gz\",\n",
    "    \"quantity\": \"claims.quantity.tsv.gz\",\n",
    "    \"time\": \"claims.time.tsv.gz\",\n",
    "    \"label\": \"labels.en.tsv.gz\",\n",
    "    \"alias\": \"aliases.en.tsv.gz\",\n",
    "    \"description\": \"descriptions.en.tsv.gz\",\n",
    "    \"item\": \"claims.wikibase-item.tsv.gz\",\n",
    "    \"external_id\": \"claims.external-id.tsv.gz\",\n",
    "    \"qualifiers\": \"qualifiers.tsv.gz\",\n",
    "    \"sitelinks\": \"sitelinks.tsv.gz\",\n",
    "    \"qualifiers_time\": \"qualifiers.time.tsv.gz\",\n",
    "    \"property_datatypes\": \"metadata.property.datatypes.tsv.gz\",\n",
    "    \"isa\": \"derived.isa.tsv.gz\",\n",
    "    \"p279star\": \"derived.P279star.tsv.gz\",\n",
    "    \"p279\": \"derived.P279.tsv.gz\",\n",
    "    \"p31\": \"derived.P31.tsv.gz\",\n",
    "    \"dwd_isa\": \"derived.dwd_isa.tsv.gz\"\n",
    "}\n",
    "\n",
    "# We will define environment variables to hold the full paths to the files as we will use them in the shell commands\n",
    "kgtk_environment_variables = []\n",
    "\n",
    "os.environ['WIKIDATA'] = wikidata_folder\n",
    "kgtk_environment_variables.append('WIKIDATA')\n",
    "\n",
    "for key, value in file_names.items():\n",
    "    variable = key.upper()\n",
    "    os.environ[variable] = wikidata_folder + value\n",
    "    kgtk_environment_variables.append(variable)\n",
    "\n",
    "os.environ[\"WD2DB\"] = \"/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-dbpedia/wikidata_to_dbpedia_edge_file.tsv.gz\"\n",
    "kgtk_environment_variables.append(\"WD2DB\")\n",
    "\n",
    "\n",
    "# KGTK creates a SQLite database to index the knowledge graph.\n",
    "if cache_path:\n",
    "    os.environ['STORE'] = \"{}/wikidata.sqlite3.db\".format(cache_path)\n",
    "else:\n",
    "    os.environ['STORE'] = \"{}/{}/wikidata.sqlite3.db\".format(output_path, temp_folder)\n",
    "kgtk_environment_variables.append('STORE')\n",
    "\n",
    "# We will create many temporary files, so set up a folder for outputs and one for the temporary files.\n",
    "os.environ['TEMP'] = \"{}/{}\".format(output_path, temp_folder) \n",
    "os.environ['OUT'] = \"{}/{}\".format(output_path, output_folder) \n",
    "kgtk_environment_variables.append('TEMP')\n",
    "kgtk_environment_variables.append('OUT')\n",
    "\n",
    "# Envronment variables with shortcuts to the commands we use often\n",
    "os.environ['kgtk'] = kgtk\n",
    "# Use for debugging, but careful as it causes import to dataframes to break\n",
    "os.environ['kypher'] = \"time kgtk --debug query --graph-cache \" + os.environ['STORE']\n",
    "# os.environ['kypher'] = \"kgtk query --graph-cache \" + os.environ['STORE']\n",
    "kgtk_environment_variables.append('kgtk')\n",
    "kgtk_environment_variables.append('kypher')\n",
    "\n",
    "# We'll save the current working directory so we can call into other example notebooks later\n",
    "os.environ[\"EXAMPLES_DIR\"] = os.getcwd()\n",
    "kgtk_environment_variables.append('EXAMPLES_DIR')\n",
    "\n",
    "kgtk_environment_variables.sort()\n",
    "for variable in kgtk_environment_variables:\n",
    "    print(\"{}: \\\"{}\\\"\".format(variable, os.environ[variable]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pedroszekely/Downloads/kypher\n"
     ]
    }
   ],
   "source": [
    "%cd {output_path}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!$kgtk add-id -i \"$OUT\"/wikidata_infobox_raw.tsv.gz --id-style wikidata -o \"$OUT\"/wikidata_infobox.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the shortcuts for Kypher so that import the relevant files into the Kypher index and define shortcuts to make the queries nicer to write"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!$kypher \\\n",
    "-i \"$ITEM\" --as items \\\n",
    "-i \"$TIME\" --as time \\\n",
    "-i \"$P31\" --as p31 \\\n",
    "-i \"$P279\" --as p279 \\\n",
    "-i \"$LABEL\" --as labels \\\n",
    "-i \"$ALIAS\" --as aliases \\\n",
    "-i \"$P279STAR\" --as p279star \\\n",
    "-i \"$QUALIFIERS\" --as qualifiers \\\n",
    "-i \"$DESCRIPTION\" --as descriptions \\\n",
    "-i \"$EXTERNAL_ID\" --as external_ids \\\n",
    "-i \"$WD2DB\" --as wd2db \\\n",
    "--limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-07-24 18:19:54 sqlstore]: IMPORT graph directly into table graph_1 from /Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/claims.wikibase-item.tsv.gz ...\n",
      "[2021-07-24 19:12:25 sqlstore]: IMPORT graph directly into table graph_2 from /Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/claims.time.tsv.gz ...\n",
      "[2021-07-24 19:18:12 sqlstore]: IMPORT graph directly into table graph_3 from /Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/derived.P31.tsv.gz ...\n",
      "[2021-07-24 19:24:51 sqlstore]: IMPORT graph directly into table graph_4 from /Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/derived.P279.tsv.gz ...\n",
      "[2021-07-24 19:25:20 sqlstore]: IMPORT graph directly into table graph_5 from /Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/labels.en.tsv.gz ...\n",
      "[2021-07-24 19:34:51 sqlstore]: IMPORT graph directly into table graph_6 from /Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/derived.P279star.tsv.gz ...\n",
      "[2021-07-24 19:41:35 sqlstore]: IMPORT graph directly into table graph_7 from /Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20210215/data/claims.external-id.tsv.gz ...\n",
      "[2021-07-24 20:00:47 sqlstore]: IMPORT graph via csv.reader into table graph_8 from /Users/pedroszekely/Downloads/kypher/wd-workshop/ulan.tsv ...\n",
      "[2021-07-24 20:00:47 query]: SQL Translation:\n",
      "---------------------------------------------\n",
      "  SELECT *\n",
      "     FROM graph_1 AS graph_1_c1\n",
      "     LIMIT ?\n",
      "  PARAS: [10]\n",
      "---------------------------------------------\n",
      "id\tnode1\tlabel\tnode2\trank\tnode2;wikidatatype\n",
      "P10-P1629-Q34508-bcc39400-0\tP10\tP1629\tQ34508\tnormal\twikibase-item\n",
      "P10-P1855-Q15075950-7eff6d65-0\tP10\tP1855\tQ15075950\tnormal\twikibase-item\n",
      "P10-P1855-Q4504-a69d2c73-0\tP10\tP1855\tQ4504\tnormal\twikibase-item\n",
      "P10-P1855-Q69063653-c8cdb04c-0\tP10\tP1855\tQ69063653\tnormal\twikibase-item\n",
      "P10-P1855-Q7378-555592a4-0\tP10\tP1855\tQ7378\tnormal\twikibase-item\n",
      "P10-P2302-Q21502404-d012aef4-0\tP10\tP2302\tQ21502404\tnormal\twikibase-item\n",
      "P10-P2302-Q21510851-5224fe0b-0\tP10\tP2302\tQ21510851\tnormal\twikibase-item\n",
      "P10-P2302-Q21510852-dde2f0ce-0\tP10\tP2302\tQ21510852\tnormal\twikibase-item\n",
      "P10-P2302-Q52004125-d0288d06-0\tP10\tP2302\tQ52004125\tnormal\twikibase-item\n",
      "P10-P2302-Q53869507-974ce3b1-0\tP10\tP2302\tQ53869507\tnormal\twikibase-item\n",
      "     6055.28 real      9659.85 user       249.56 sys\n"
     ]
    }
   ],
   "source": [
    "!$kypher \\\n",
    "-i \"$ITEM\" --as items \\\n",
    "-i \"$TIME\" --as time \\\n",
    "-i \"$P31\" --as p31 \\\n",
    "-i \"$P279\" --as p279 \\\n",
    "-i \"$LABEL\" --as labels \\\n",
    "-i \"$P279STAR\" --as p279star \\\n",
    "-i \"$EXTERNAL_ID\" --as external_ids \\\n",
    "-i \"$OUT\"/ulan.tsv --as ulan \\\n",
    "--limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve large amounts of data from Wikidata\n",
    "\n",
    "John is doing research on the popularity of first names to improve his entity resolution algorithm for people. He sees that Wikidata contains about 9 million people, so he wants to get the distribution of counts of first names from Wikidata. He writes a SPARQL query, but it times out, so he downloads the Wikidata KGTK files on his laptop and writes a kypher query. The query retrieves all instances of human (Q5), gets their frst names using the P735 property and return the counts.\n",
    "\n",
    "John thinks he will want to do additional analysis on the data, so chooses standard KGTK names for the headers to generate the data as a KGTK graph that then he can use as input to other KGTK commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-07-24 20:00:49 query]: SQL Translation:\n",
      "---------------------------------------------\n",
      "  SELECT DISTINCT graph_1_c2.\"node2\" \"_aLias.node1\", count(graph_1_c2.\"node2\") \"_aLias.node2\", graph_5_c3.\"node2\" \"_aLias.node1;label\", ? \"_aLias.label\"\n",
      "     FROM graph_1 AS graph_1_c2\n",
      "     INNER JOIN graph_3 AS graph_3_c1, graph_5 AS graph_5_c3\n",
      "     ON graph_1_c2.\"node2\" = graph_5_c3.\"node1\"\n",
      "        AND graph_3_c1.\"node1\" = graph_1_c2.\"node1\"\n",
      "        AND graph_1_c2.\"label\" = ?\n",
      "        AND graph_3_c1.\"node2\" = ?\n",
      "     GROUP BY \"_aLias.node1\"\n",
      "     ORDER BY \"_aLias.node2\" DESC\n",
      "  PARAS: ['count_names', 'P735', 'Q5']\n",
      "---------------------------------------------\n",
      "[2021-07-24 20:00:49 sqlstore]: CREATE INDEX on table graph_1 column node1 ...\n",
      "[2021-07-24 20:07:24 sqlstore]: ANALYZE INDEX on table graph_1 column node1 ...\n",
      "[2021-07-24 20:08:19 sqlstore]: CREATE INDEX on table graph_5 column node1 ...\n",
      "[2021-07-24 20:09:13 sqlstore]: ANALYZE INDEX on table graph_5 column node1 ...\n",
      "[2021-07-24 20:09:19 sqlstore]: CREATE INDEX on table graph_1 column node2 ...\n",
      "[2021-07-24 20:21:18 sqlstore]: ANALYZE INDEX on table graph_1 column node2 ...\n",
      "[2021-07-24 20:22:07 sqlstore]: CREATE INDEX on table graph_1 column label ...\n",
      "[2021-07-24 20:30:13 sqlstore]: ANALYZE INDEX on table graph_1 column label ...\n",
      "[2021-07-24 20:30:55 sqlstore]: CREATE INDEX on table graph_3 column node2 ...\n",
      "[2021-07-24 20:32:09 sqlstore]: ANALYZE INDEX on table graph_3 column node2 ...\n",
      "[2021-07-24 20:32:15 sqlstore]: CREATE INDEX on table graph_3 column node1 ...\n",
      "[2021-07-24 20:32:56 sqlstore]: ANALYZE INDEX on table graph_3 column node1 ...\n",
      "     2392.06 real      1326.90 user       316.47 sys\n"
     ]
    }
   ],
   "source": [
    "# compare to SPARQL\n",
    "# common names\n",
    "!$kypher -i items -i p31 -i labels \\\n",
    "--match '\\\n",
    "    p31: (person)-[]->(:Q5), \\\n",
    "    items: (person)-[:P735]->(given_name), \\\n",
    "    labels: (given_name)-[]->(given_name_label)' \\\n",
    "--return 'distinct given_name as node1, count(given_name) as node2, given_name_label as `node1;label`, \"count_names\" as label' \\\n",
    "--order-by 'node2 desc' \\\n",
    "-o \"$OUT\"/given-names.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   53253  216696 1988754 /Users/pedroszekely/Downloads/kypher/wd-workshop/given-names.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc \"$OUT\"/given-names.tsv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "John takes a peek at the file to make sure he got the headers correcly: an edge from the q-node to the count, using `count_names` as the property, and including the `label` of `node1` so he can read the data. John sees that his name is by far the most popular name in Wikidata, and gets the information he needs to fine tune his entity resolution algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1      node2   node1;label   label\n",
      "Q4925477   120416  'John'@en     count_names\n",
      "Q12344159  74235   'William'@en  count_names\n",
      "Q4927937   59298   'Robert'@en   count_names\n",
      "Q16428906  57107   'Thomas'@en   count_names\n",
      "Q677191    52568   'James'@en    count_names\n",
      "Q18057751  49005   'David'@en    count_names\n",
      "Q2958359   44735   'Charles'@en  count_names\n",
      "Q2793400   40987   'Peter'@en    count_names\n",
      "Q1249148   40149   'Richard'@en  count_names\n"
     ]
    }
   ],
   "source": [
    "!head \"$OUT\"/given-names.tsv | column -ts $'\\t'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "John gets curious and wants to know whether the popularity of names depends of time, so modifies his query to partition the data by people's year of birth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-07-24 20:40:41 query]: SQL Translation:\n",
      "---------------------------------------------\n",
      "  SELECT DISTINCT graph_1_c2.\"node2\" \"_aLias.node1\", kgtk_date_year(graph_2_c3.\"node2\") \"_aLias.year\", count(graph_1_c2.\"node2\") \"_aLias.node2\", graph_5_c4.\"node2\" \"_aLias.node1;label\", ? \"_aLias.label\"\n",
      "     FROM graph_1 AS graph_1_c2\n",
      "     INNER JOIN graph_2 AS graph_2_c3, graph_3 AS graph_3_c1, graph_5 AS graph_5_c4\n",
      "     ON graph_1_c2.\"node2\" = graph_5_c4.\"node1\"\n",
      "        AND graph_3_c1.\"node1\" = graph_1_c2.\"node1\"\n",
      "        AND graph_3_c1.\"node1\" = graph_2_c3.\"node1\"\n",
      "        AND graph_1_c2.\"label\" = ?\n",
      "        AND graph_2_c3.\"label\" = ?\n",
      "        AND graph_3_c1.\"node2\" = ?\n",
      "     GROUP BY \"_aLias.node1\", \"_aLias.year\"\n",
      "     ORDER BY graph_1_c2.\"node2\" ASC, CAST(\"_aLias.year\" AS integer) ASC, \"_aLias.node2\" DESC\n",
      "  PARAS: ['count_names_yearly', 'P735', 'P569', 'Q5']\n",
      "---------------------------------------------\n",
      "[2021-07-24 20:40:41 sqlstore]: CREATE INDEX on table graph_2 column node1 ...\n",
      "[2021-07-24 20:41:17 sqlstore]: ANALYZE INDEX on table graph_2 column node1 ...\n",
      "[2021-07-24 20:41:21 sqlstore]: CREATE INDEX on table graph_2 column label ...\n",
      "[2021-07-24 20:41:49 sqlstore]: ANALYZE INDEX on table graph_2 column label ...\n",
      "      555.93 real       128.42 user        90.71 sys\n"
     ]
    }
   ],
   "source": [
    "!$kypher -i items -i time -i p31 -i labels \\\n",
    "--match '\\\n",
    "    p31: (person)-[]->(:Q5), \\\n",
    "    items: (person)-[:P735]->(given_name), \\\n",
    "    time: (person)-[:P569]->(date_of_birth), \\\n",
    "    labels: (given_name)-[]->(given_name_label)' \\\n",
    "--return 'distinct given_name as node1, kgtk_date_year(date_of_birth) as year, count(given_name) as node2, given_name_label as `node1;label`, \"count_names_yearly\" as label' \\\n",
    "--order-by 'given_name, cast(year, integer), node2 desc' \\\n",
    "-o \"$OUT\"/given-names.year.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  882179 4431755 42410662 /Users/pedroszekely/Downloads/kypher/wd-workshop/given-names.year.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc \"$OUT\"/given-names.year.tsv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "John takea a quick peek at the file to verify that the headers are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1     year  node2  node1;label      label\n",
      "Q1000387  1798  1      'Ferdinanda'@en  count_names_yearly\n",
      "Q1000387  1849  1      'Ferdinanda'@en  count_names_yearly\n",
      "Q1000387  1868  1      'Ferdinanda'@en  count_names_yearly\n",
      "Q1000387  1870  1      'Ferdinanda'@en  count_names_yearly\n"
     ]
    }
   ],
   "source": [
    "!head -5 \"$OUT\"/given-names.year.tsv | column -ts $'\\t'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "John heard anecdotaly that Jessica had become a popular name in the late 90s and greps for Jessica in the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q630846\t1995\t58\t'Jessica'@en\tcount_names_yearly\n",
      "Q630846\t1996\t41\t'Jessica'@en\tcount_names_yearly\n",
      "Q630846\t1997\t27\t'Jessica'@en\tcount_names_yearly\n",
      "Q630846\t1998\t23\t'Jessica'@en\tcount_names_yearly\n",
      "Q630846\t1999\t23\t'Jessica'@en\tcount_names_yearly\n",
      "Q630846\t2000\t51\t'Jessica'@en\tcount_names_yearly\n",
      "Q630846\t2001\t18\t'Jessica'@en\tcount_names_yearly\n",
      "Q630846\t2002\t18\t'Jessica'@en\tcount_names_yearly\n",
      "Q630846\t2003\t11\t'Jessica'@en\tcount_names_yearly\n",
      "Q630846\t2004\t6\t'Jessica'@en\tcount_names_yearly\n",
      "Q630846\t2005\t2\t'Jessica'@en\tcount_names_yearly\n",
      "Q630846\t2009\t1\t'Jessica'@en\tcount_names_yearly\n",
      "Q630846\t2011\t1\t'Jessica'@en\tcount_names_yearly\n",
      "Q630846\t2014\t1\t'Jessica'@en\tcount_names_yearly\n",
      "Q630846\t2016\t2\t'Jessica'@en\tcount_names_yearly\n"
     ]
    }
   ],
   "source": [
    "!grep \"'Jessica'\" \"$OUT\"/given-names.year.tsv | tail -15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "John realizes that he needs to normalize the counts of names by the number of people born in each year. He wonders whether he can do it in one kypher query, but takes the easy way out and writes a simple query to get the counts of people born each year. He can do this faster than he can think of a complex query to get the final result in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-07-24 20:49:59 query]: SQL Translation:\n",
      "---------------------------------------------\n",
      "  SELECT kgtk_date_year(graph_2_c2.\"node2\") \"_aLias.node1\", count(graph_3_c1.\"node1\") \"_aLias.node2\", ? \"_aLias.label\"\n",
      "     FROM graph_2 AS graph_2_c2\n",
      "     INNER JOIN graph_3 AS graph_3_c1\n",
      "     ON graph_3_c1.\"node1\" = graph_2_c2.\"node1\"\n",
      "        AND graph_2_c2.\"label\" = ?\n",
      "        AND graph_3_c1.\"node2\" = ?\n",
      "     GROUP BY \"_aLias.node1\"\n",
      "  PARAS: ['count_people_born', 'P569', 'Q5']\n",
      "---------------------------------------------\n",
      "       84.38 real        33.21 user        15.57 sys\n"
     ]
    }
   ],
   "source": [
    "!$kypher -i time -i p31 \\\n",
    "--match ' \\\n",
    "    p31: (person)-[]->(:Q5), \\\n",
    "    time: (person)-[:P569]->(date_of_birth)' \\\n",
    "--return 'kgtk_date_year(date_of_birth) as node1, count(person) as node2, \"count_people_born\" as label' \\\n",
    "-o \"$TEMP\"/human.count.year.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "John is happy that KGTK accepts literals as subjects of triples because here the subjects (`node1`) are years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1\tnode2\tlabel\n",
      "1\t105\tcount_people_born\n",
      "2\t5\tcount_people_born\n",
      "3\t9\tcount_people_born\n",
      "4\t8\tcount_people_born\n",
      "5\t11\tcount_people_born\n",
      "6\t10\tcount_people_born\n",
      "7\t7\tcount_people_born\n",
      "8\t5\tcount_people_born\n",
      "9\t9\tcount_people_born\n"
     ]
    }
   ],
   "source": [
    "!head \"$TEMP\"/human.count.year.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "John knows he is almost there. He needs to get the names from the `given-names.year.tsv` file, and needs to pick out the year from the qualifier he put on the edge using the syntax to get the attributes of edges `[r {year: the_year}]`. He computes the fraction of people with each name and multiplies by 100,000 so that the numbers are not so tiny and easier to read. John also gets the labels of the q-nodes from the attribute he put on `node1` so tha the doesn't have to join with the `labels.tsv` file again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-07-24 20:51:24 sqlstore]: IMPORT graph directly into table graph_9 from /Users/pedroszekely/Downloads/kypher/wd-workshop/given-names.year.tsv ...\n",
      "[2021-07-24 20:51:28 sqlstore]: IMPORT graph directly into table graph_10 from /Users/pedroszekely/Downloads/kypher/temp.wd-workshop/human.count.year.tsv ...\n",
      "[2021-07-24 20:51:28 query]: SQL Translation:\n",
      "---------------------------------------------\n",
      "  SELECT graph_9_c1.\"node1\" \"_aLias.node1\", ? \"_aLias.label\", (CAST(graph_9_c1.\"node2\" AS float) * (? / CAST(graph_10_c2.\"node2\" AS float))) \"_aLias.node2\", graph_10_c2.\"node1\" \"_aLias.year\", graph_9_c1.\"node1;label\" \"_aLias.node1;label\"\n",
      "     FROM graph_10 AS graph_10_c2\n",
      "     INNER JOIN graph_9 AS graph_9_c1\n",
      "     ON graph_10_c2.\"node1\" = graph_9_c1.\"year\"\n",
      "        AND graph_9_c1.\"node1;label\" = graph_9_c1.\"node1;label\"\n",
      "        AND graph_9_c1.\"year\" = graph_10_c2.\"node1\"\n",
      "     ORDER BY graph_9_c1.\"node1\" ASC, CAST(graph_10_c2.\"node1\" AS integer) ASC, \"_aLias.node2\" DESC\n",
      "  PARAS: ['normalized_count_names_yearly', 10000]\n",
      "---------------------------------------------\n",
      "[2021-07-24 20:51:28 sqlstore]: CREATE INDEX on table graph_10 column node1 ...\n",
      "[2021-07-24 20:51:28 sqlstore]: ANALYZE INDEX on table graph_10 column node1 ...\n",
      "        9.69 real        10.39 user         0.57 sys\n"
     ]
    }
   ],
   "source": [
    "!$kypher -i \"$OUT\"/given-names.year.tsv -i \"$TEMP\"/human.count.year.tsv \\\n",
    "--match ' \\\n",
    "    names: (given_name {label: given_name_label})-[r {year: the_year}]->(count_names), \\\n",
    "    year: (the_year)-[]->(count_people)' \\\n",
    "--return 'given_name as node1, \"normalized_count_names_yearly\" as label, cast(count_names, float) * 10000 / cast(count_people, float) as node2, the_year as year, given_name_label as `node1;label`' \\\n",
    "--order-by 'given_name, cast(the_year, integer), node2 desc' \\\n",
    "-o \"$OUT\"/given-names.year.normalized.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1     label                          node2               year  node1;label\n",
      "Q1000387  normalized_count_names_yearly  2.207505518763797   1798  'Ferdinanda'@en\n",
      "Q1000387  normalized_count_names_yearly  1.1767474699929394  1849  'Ferdinanda'@en\n",
      "Q1000387  normalized_count_names_yearly  0.6827336655970506  1868  'Ferdinanda'@en\n",
      "Q1000387  normalized_count_names_yearly  0.6334726973267453  1870  'Ferdinanda'@en\n",
      "Q1000387  normalized_count_names_yearly  0.536711034778875   1888  'Ferdinanda'@en\n",
      "Q1000433  normalized_count_names_yearly  1.0892059688487092  1852  'Bud'@en\n",
      "Q1000433  normalized_count_names_yearly  0.9004141905276427  1858  'Bud'@en\n",
      "Q1000433  normalized_count_names_yearly  0.6082355087890031  1881  'Bud'@en\n",
      "Q1000433  normalized_count_names_yearly  0.5845218611176058  1882  'Bud'@en\n"
     ]
    }
   ],
   "source": [
    "!head \"$OUT\"/given-names.year.normalized.tsv | column -ts $'\\t'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "John greps the normalized file again. Jessica was not a popular name in the 60s and began to get popular in the late 70s. John satisfied his curiosity. The popularity of names is time dependent, but for now, John will work to integrate the aggregate data into his entity resolution algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q630846\tnormalized_count_names_yearly\t1.5782828282828283\t1960\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t1.6427682992654478\t1961\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t0.6936416184971098\t1962\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t2.2975301550832854\t1963\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t2.7612232218872963\t1964\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t3.534651365553644\t1965\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t3.7685601587820012\t1966\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t2.2576760987357014\t1967\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t3.7844383893430216\t1968\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t4.271034846619601\t1969\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t5.083022704168078\t1970\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t8.008208413623965\t1971\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t5.099569086412198\t1972\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t6.0163750032697685\t1973\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t5.754944020089986\t1974\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t5.728569940631185\t1975\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t9.301089556205154\t1976\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t10.779346771585644\t1977\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t12.160190239420636\t1978\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t10.66564568178089\t1979\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t15.91069146300112\t1980\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t15.169066137128357\t1981\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t17.050067658998646\t1982\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t17.04045734388742\t1983\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t14.809126810004388\t1984\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t18.211965533175675\t1985\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t16.454134101192924\t1986\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t15.175359712230215\t1987\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t21.840097312838658\t1988\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t19.4208031073285\t1989\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t16.48898365316276\t1990\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t15.925029859430985\t1991\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t20.21772939346812\t1992\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t17.561880778024452\t1993\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t20.131709147985124\t1994\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t21.767686245074124\t1995\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t16.636234530330697\t1996\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t11.965433192998006\t1997\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t12.055139158236805\t1998\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t14.090547080806223\t1999\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t9.757217471158812\t2000\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t17.07455890722823\t2001\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t22.304832713754646\t2002\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t21.202775636083267\t2003\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t17.846519928613922\t2004\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t12.106537530266344\t2005\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t15.19756838905775\t2009\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t20.74688796680498\t2011\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t25.188916876574307\t2014\t'Jessica'@en\n",
      "Q630846\tnormalized_count_names_yearly\t64.72491909385113\t2016\t'Jessica'@en\n"
     ]
    }
   ],
   "source": [
    "!grep \"'Jessica'\" \"$OUT\"/given-names.year.normalized.tsv | tail -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytics on full Wikidata\n",
    "\n",
    "Jessica is working with John on the entity resolution algorithm and her job is to use the number of instances of each class in Wikidata as a feature. The query that Jessica needs to write is simple as she just needs to count the number of instances of each class, summing up over the instances of all subclasses. She knows that there are over 1 million classes in Wikidata (entities with a P279 property), so she knows it will not run on the public SPARQL endpint. Jessica gets the SQLite database from John so that she does not have to wait the 2 or so hours to load it on her laptop, writes the query and goes for lunch as she knows it will take a while for it to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-07-24 20:51:34 query]: SQL Translation:\n",
      "---------------------------------------------\n",
      "  SELECT DISTINCT graph_6_c2.\"node2\" \"_aLias.node1\", count(DISTINCT graph_3_c1.\"node1\") \"_aLias.node2\", ? \"_aLias.label\"\n",
      "     FROM graph_3 AS graph_3_c1\n",
      "     INNER JOIN graph_6 AS graph_6_c2\n",
      "     ON graph_3_c1.\"node2\" = graph_6_c2.\"node1\"\n",
      "     GROUP BY \"_aLias.node1\"\n",
      "     ORDER BY \"_aLias.node2\" DESC, \"_aLias.node1\" ASC\n",
      "  PARAS: ['entity_count']\n",
      "---------------------------------------------\n",
      "[2021-07-24 20:51:34 sqlstore]: CREATE INDEX on table graph_6 column node1 ...\n",
      "[2021-07-24 20:52:27 sqlstore]: ANALYZE INDEX on table graph_6 column node1 ...\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# compare to SPARQL\n",
    "!$kypher -i p31 -i p279star \\\n",
    "--match '\\\n",
    "    p31: (entity)-[]->(class), \\\n",
    "    p279star: (class)-[]->(super_class)' \\\n",
    "--return 'distinct super_class as node1, count(distinct entity) as node2, \"entity_count\" as label' \\\n",
    "--order-by 'node2 desc, node1' \\\n",
    "-o \"$OUT\"/class.count.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After coming back from lunch, the file is ready, it contains data for 75K classes, she figures that the other classes don't have instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zcat: (stdin): unexpected end of file\n",
      "       0       0       0\n"
     ]
    }
   ],
   "source": [
    "!zcat < \"$OUT\"/class.count.tsv.gz | wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zcat: (stdin): unexpected end of file\n"
     ]
    }
   ],
   "source": [
    "!zcat < \"$OUT\"/class.count.tsv.gz | head -5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jessica is curious about the data, so she writes a query to get the counts of different classes of film (Q11424). Jessica had been working with John, so she learned the trick to use the standard names for column headings so that she can use the output of previous queries as new graphs. She shudders to think that if she was using SPARQL she would have had to set up a new Wikidata SPARQL endpoint to be able to load her personal data in it, and to be extremely caeful to not make a mistake because deleting the data would have been a chore. Jessica had watched John make several mistakes when he was building the files for the names. John had simply fixed the queries and re-run the other queries that depended on the data he had just fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 568, in _build_master\n",
      "    ws.require(__requires__)\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 886, in require\n",
      "    needed = self.resolve(parse_requirements(requirements))\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 777, in resolve\n",
      "    raise VersionConflict(dist, req).with_context(dependent_req)\n",
      "pkg_resources.ContextualVersionConflict: (etk 2.2.3a0 (/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages), Requirement.parse('etk>=2.2.6'), {'kgtk'})\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 3016, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 2813, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/_vendor/pyparsing.py\", line 3552, in parseImpl\n",
      "    ret = e._parse( instring, loc, doActions )\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/_vendor/pyparsing.py\", line 1402, in _parseNoCache\n",
      "    loc,tokens = self.parseImpl( instring, preloc, doActions )\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/_vendor/pyparsing.py\", line 3400, in parseImpl\n",
      "    loc, resultlist = self.exprs[0]._parse( instring, loc, doActions, callPreParse=False )\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/_vendor/pyparsing.py\", line 1406, in _parseNoCache\n",
      "    loc,tokens = self.parseImpl( instring, preloc, doActions )\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/_vendor/pyparsing.py\", line 3739, in parseImpl\n",
      "    return self.expr._parse( instring, loc, doActions, callPreParse=False )\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/_vendor/pyparsing.py\", line 1406, in _parseNoCache\n",
      "    loc,tokens = self.parseImpl( instring, preloc, doActions )\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/_vendor/pyparsing.py\", line 2435, in parseImpl\n",
      "    raise ParseException(instring, loc, self.errmsg, self)\n",
      "pkg_resources._vendor.pyparsing.ParseException: Expected \"@\" (at char 6), (line:1, col:7)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/bin/kgtk\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('kgtk==0.7.1', 'console_scripts', 'kgtk')())\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/kgtk-0.7.1-py3.7.egg/kgtk/cli_entry.py\", line 300, in cli_entry\n",
      "    mod = importlib.import_module('.{}'.format(h), 'kgtk.cli')\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/kgtk-0.7.1-py3.7.egg/kgtk/cli/graph_embeddings.py\", line 8, in <module>\n",
      "    from kgtk.io.kgtkreader import KgtkReader, KgtkReaderMode, KgtkReaderOptions\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/kgtk-0.7.1-py3.7.egg/kgtk/io/kgtkreader.py\", line 29, in <module>\n",
      "    from kgtk.io.kgtkbase import KgtkBase\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/kgtk-0.7.1-py3.7.egg/kgtk/io/kgtkbase.py\", line 14, in <module>\n",
      "    from kgtk.value.kgtkvalue import KgtkValue\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/kgtk-0.7.1-py3.7.egg/kgtk/value/kgtkvalue.py\", line 15, in <module>\n",
      "    from kgtk.value.languagevalidator import LanguageValidator\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/kgtk-0.7.1-py3.7.egg/kgtk/value/languagevalidator.py\", line 21, in <module>\n",
      "    import pycountry # type: ignore\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pycountry/__init__.py\", line 10, in <module>\n",
      "    import pkg_resources\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 3242, in <module>\n",
      "    @_call_aside\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 3226, in _call_aside\n",
      "    f(*args, **kwargs)\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 3255, in _initialize_master_working_set\n",
      "    working_set = WorkingSet._build_master()\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 570, in _build_master\n",
      "    return cls._build_from_requirements(__requires__)\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 583, in _build_from_requirements\n",
      "    dists = ws.resolve(reqs, Environment())\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 780, in resolve\n",
      "    new_requirements = dist.requires(req.extras)[::-1]\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 2734, in requires\n",
      "    dm = self._dep_map\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 3018, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 3028, in _compute_dependencies\n",
      "    reqs.extend(parse_requirements(req))\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 3084, in parse_requirements\n",
      "    yield Requirement(line)\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 3094, in __init__\n",
      "    super(Requirement, self).__init__(requirement_string)\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/_vendor/packaging/requirements.py\", line 98, in __init__\n",
      "    req = REQUIREMENT.parseString(requirement_string)\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/_vendor/pyparsing.py\", line 1644, in parseString\n",
      "    loc, tokens = self._parse( instring, 0 )\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/_vendor/pyparsing.py\", line 1402, in _parseNoCache\n",
      "    loc,tokens = self.parseImpl( instring, preloc, doActions )\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/_vendor/pyparsing.py\", line 3417, in parseImpl\n",
      "    loc, exprtokens = e._parse( instring, loc, doActions )\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/_vendor/pyparsing.py\", line 1402, in _parseNoCache\n",
      "    loc,tokens = self.parseImpl( instring, preloc, doActions )\n",
      "  File \"/Users/pedroszekely/opt/anaconda3/envs/kgtk-env/lib/python3.7/site-packages/pkg_resources/_vendor/pyparsing.py\", line 3552, in parseImpl\n",
      "    ret = e._parse( instring, loc, doActions )\n",
      "KeyboardInterrupt\n",
      "        1.06 real         0.68 user         0.14 sys\n"
     ]
    }
   ],
   "source": [
    "!$kypher -i p279star -i labels -i \"$OUT\"/class.count.tsv.gz \\\n",
    "--match ' \\\n",
    "    p279star: (class)-[]->(:Q11424), \\\n",
    "    count: (class)-[]->(count), \\\n",
    "    labels: (class)-[]->(class_label)' \\\n",
    "--return 'class as class, class_label as name, count as count' \\\n",
    "--order-by 'cast(count, integer) desc' \\\n",
    "--limit 10 \\\n",
    "| column -ts $'\\t'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jessica now has the statistics she needs to work on her feature for the entity resolution algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract new graphs from Wikidata\n",
    "\n",
    "Bill is working on a project to find networks of researchers working on specific topics. He wants to use publication data to find relationships among authors using publications. Bill knows that he can get lots of publication data from Pubmed or Microsoft Academic graph, but wants to give Wikidata a try as he heard that Wikidata has close to 40 million publications, and that in Wikidata publications have links to other entities such as main subjects.\n",
    "\n",
    "Bill decides that the simplest experiment to try first is to build a network of authors of publications in Wikidata: he wants to create a graph of people in Wikidata who authored papers, to put a link between two people if the coauthored a paper, and to add a qualifier with the count of papers they coauthored. He knows the computation is expensive as there are 40ish million papers in Wikidata, so the network will be large. He doesn't even try to write a SPARQL query because he knows it will time out. Bill downloads the KGTK files and decides to write his first query using only 2019 data so he doesn't have to wait so long if he makes a mistake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First do it for 2019 to debug the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$kypher -i p31 -i p279star -i items -i time -i labels \\\n",
    "--match '\\\n",
    "    p31: (pub)-[]->(class), \\\n",
    "    p279star: (class)-[]->(:Q591041), \\\n",
    "    time: (pub)-[:P577]->(pub_date), \\\n",
    "    items: (pub)-[:P50]->(author1), \\\n",
    "    items: (pub)-[:P50]->(author2), \\\n",
    "    labels: (author1)-[]->(author1_label)' \\\n",
    "--where 'author1 != author2 and kgtk_date_year(pub_date) = 2019' \\\n",
    "--return 'distinct author1 as node1, \"Pcoauthor\" as relation, author2 as node2, count(distinct pub) as count_publications, author1_label as `node1;label`' \\\n",
    "--order-by 'count_publications desc' \\\n",
    "-o \"$TEMP\"/coauthors.2019.id.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zcat < \"$TEMP\"/coauthors.2019.id.tsv.gz | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bill wants to sanity check his data so he looks up the first person in Google Scholar and finds that Secundino Lpez Puente has many publications in 2019. Looks like the query is working fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network for all authors\n",
    "\n",
    "Bill removes the year restriction and runs the query for the full data. The query for a single year took close to 10 minutes, so Bill decides to leave the query running overnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to SPARQL\n",
    "!$kypher -i p31 -i p279star -i items -i time -i labels \\\n",
    "--match '\\\n",
    "    p31: (pub)-[]->(class), \\\n",
    "    p279star: (class)-[]->(:Q591041), \\\n",
    "    time: (pub)-[:P577]->(pub_date), \\\n",
    "    items: (pub)-[:P50]->(author1), \\\n",
    "    items: (pub)-[:P50]->(author2)' \\\n",
    "--where 'author1 != author2' \\\n",
    "--return 'distinct author1 as node1, \"Pcoauthor\" as relation, author2 as node2, count(distinct pub) as count_publications' \\\n",
    "--order-by 'count_publications desc' \\\n",
    "-o \"$TEMP\"/coauthors.2019.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a network of authors who authored papers about cancer\n",
    "\n",
    "Bill is interested in cancer research, so he wants to build the same network but using only the papers about cancer. He knows Wikidata has an extensive class hiearchy, so he writes a query to peek at the hierarchy below the q-node for cancer.\n",
    "He writes a query to retrieve subclasses of cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$kypher -i p279star -i labels \\\n",
    "--match '\\\n",
    "    p279star: (cancer_type)-[]->(:Q12078), \\\n",
    "    labels: (cancer_type)-[]->(cancer_type_label)' \\\n",
    "--return 'cancer_type as node1, cancer_type_label as node2' \\\n",
    "--limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are promising, so Bill now incorporates the query for types of cancer into the query for building the coauthor network. He just needs to get the main subject of the paper using the `P921` property and test that the main subject is a subclass of cancer. He expects the query to be much faster because now it has strong restriction, so he gives it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to SPARQL\n",
    "!$kypher -i p31 -i p279star -i items -i labels \\\n",
    "--match '\\\n",
    "    p31: (pub)-[]->(class), \\\n",
    "    p279star: (class)-[]->(:Q591041), \\\n",
    "    items: (pub)-[:P50]->(author1), \\\n",
    "    items: (pub)-[:P50]->(author2), \\\n",
    "    items: (pub)-[:P921]->(cancer_type), \\\n",
    "    p279star: (cancer_type)-[]->(:Q12078), \\\n",
    "    labels: (author1)-[]->(author1_label), \\\n",
    "    labels: (author2)-[]->(author2_label)' \\\n",
    "--where 'author1 != author2' \\\n",
    "--return 'distinct author1 as node_x, \"Pcoauthor\" as relation, author2 as node_y, count(distinct pub) as count_publications, author1_label as `node1;label`, author2_label as `node2;label`' \\\n",
    "--order-by 'count_publications desc' \\\n",
    "-o \"$TEMP\"/coauthors.cancer.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query takes less than a minute and produces a network with close to half a million edges. Bill takes a peek to see what is in it, and now wonders whether he could have written the query in SPARQL and run it on the public SPARQL endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zcat < \"$TEMP\"/coauthors.cancer.tsv.gz | wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zcat < \"$TEMP\"/coauthors.cancer.tsv.gz | head | column -ts $'\\t'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bill puts the first two names in Google and finds that they are famous and have publshied a lot together. Bill is happy to have a network with close to half a million edges that he can use to do interesting analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Wikidata with external files\n",
    "\n",
    "Abigail is working on a cultural heritage project, collaborating with the Getty Research Institute who gave her a file with 27 thousand ULAN identifiers. Abigail has a database indexed using VIAF identifiers, and wants to map her ULAN identifiers to VIAF identifiers so that she can use her database. She puts one of the ULAN identifiers in the Wikidata search box and discovers that Wikidata has both ULAN and VIAF identifiers for many artists. Abigail knows a little bit of SPARQL and easity figures out that it is easy to write a query to retrieve the VIAF identifier given a ULAN identifier. Her solution would require sending 27,000 queries to Wikidata, which would involve writing a Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc \"$OUT\"/ulan.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her colleague Bill tells her that she can easily solve the problem using KGTK query. The only thing she needs to do is to rename the header of her file with identifiers to `node1` and write a Kypher query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to SPARQL\n",
    "!$kypher -i items -i external_ids -i labels -i ulan \\\n",
    "--match '\\\n",
    "    ulan: (ulan_id)-[]->(), \\\n",
    "    external_ids: (viaf_id)<-[:P214]-(artist)-[:P245]->(ulan_id), \\\n",
    "    labels: (artist)-[]->(artist_label)' \\\n",
    "--return 'artist as qnode, viaf_id as viaf, ulan_id as ulan, artist_label as name' \\\n",
    "-o \"$OUT\"/ulan-to-viaf.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abigail is thrilled to see that the query ran in less than 30 seconds and is curious to see the results. She got matches for 8,116 ULAN ids, which means that now she can get a lot of data from her database to do her analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc \"$OUT\"/ulan-to-viaf.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head \"$OUT\"/ulan-to-viaf.tsv | column -ts $'\\t'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Wikidata with DBpedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After mining her VIAF database, Abigail realizes that there is a lot of interesting data in Wikipedia infoboxes that she would like to use in her analysis. She hears from a colleague that DBpedia extracts data from Wikipedia infoboxes. She is curious whether Wikidata already has most of this data. She browses the pages for some artists in Wikipedia and sees that the Wikipedia infoboxes have interesting information that she may want to include in her dataset.\n",
    "\n",
    "Abigail downloads the DBpedia infobox data in RDF format and uses KGTK to convert it into KGTK format and to substitute the DBpedia URIs with Wikidata Q-nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99251608\n"
     ]
    }
   ],
   "source": [
    "!zcat < \"$OUT\"/wikidata_infobox.tsv.gz | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abigail sees that there are almost 100 million edges in the DBpedia infobox graph, so she first adds the dataset to the kypher index.\n",
    "> This operation is similar to loading the triples into a triple store to enable running queries. In kypher it is not necessary to explicitly load the file as kypher will automatically load the file and build indices the first time the file is used. Abigail is doing separately as she is curious to see how long it takes to load the file in kypher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-07-24 21:20:28 query]: SQL Translation:\n",
      "---------------------------------------------\n",
      "  SELECT *\n",
      "     FROM graph_11 AS graph_11_c1\n",
      "     LIMIT ?\n",
      "  PARAS: [1]\n",
      "---------------------------------------------\n",
      "node1\tlabel\tnode2\tid\n",
      "nodemxZbyK2VRrGoaxfdLmyLxw-1\tdbpedia:structured_value\t\"2019-08-07\"\tnodemxZbyK2VRrGoaxfdLmyLxw-1-dbpedia:structured_value-944f0e\n",
      "        2.06 real         0.85 user         0.19 sys\n"
     ]
    }
   ],
   "source": [
    "!$kypher -i \"$OUT\"/wikidata_infobox.tsv.gz --as infobox \\\n",
    "--limit 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abigail first wants to see which properties are available for the people she has in the ULAN file. She constructs a query to count the number of statements for each property for the artists in her ULAN file.\n",
    "\n",
    "> This query combines Wikidata with two external sources, her ULAN identifiers and DBPedia infoboxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$kypher -i infobox -i ulan -i external_ids \\\n",
    "--match '\\\n",
    "    ulan: (ulan_id)-[]->(), \\\n",
    "    external_ids: (artist)-[:P245]->(ulan_id), \\\n",
    "    infobox: (artist)-[l]->()' \\\n",
    "--return 'l.label as node1, count(distinct l) as node2' \\\n",
    "--order-by 'node2 desc' \\\n",
    "--limit 20 \\\n",
    "| column -ts $'\\t'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abigail is interested in the information about spouses as she is thinking of doing an analysis on the occupation of spouses. She constructs a query to retrieve the spouse statements already present in Wikidata for her ULAN artists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$kypher -i items -i ulan -i external_ids -i labels \\\n",
    "--match '\\\n",
    "    ulan: (ulan_id)-[]->(), \\\n",
    "    external_ids: (artist)-[:P245]->(ulan_id), \\\n",
    "    items: (artist)-[l:P26]->(spouse)' \\\n",
    "--return 'artist as node1, l.label as label, spouse as node2' \\\n",
    "-o \"$OUT\"/spouses.ulan.wikidata.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc \"$OUT\"/spouses.ulan.wikidata.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "She also constructs a query to count the spouse statements of ULAN artists in the DBpedia dataset an converts the DBpedia property to `P26`, the Wikidata property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$kypher -i items -i infobox -i ulan -i external_ids -i labels \\\n",
    "--match '\\\n",
    "    ulan: (ulan_id)-[]->(), \\\n",
    "    external_ids: (artist)-[:P245]->(ulan_id), \\\n",
    "    infobox: (artist)-[:`property:spouse`]->(spouse)' \\\n",
    "--return 'artist as node1, \"P26\" as label, spouse as node2' \\\n",
    "-o \"$OUT\"/spouses.ulan.dbpedia.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abigail is encouraged as there are almost 1,000 additional statements in DBpedia that she may be able to import into her dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc \"$OUT\"/spouses.ulan.dbpedia.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abigail worries that some of the spouses may be strings rather than entities as she has seen Wikipedia infoboxes where some values are links and others are strings. She uses the regex feature in Kypher to count the number of spouses that are Wikidata q-nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$kypher -i \"$OUT\"/spouses.ulan.dbpedia.tsv \\\n",
    "--match '()-[]->(spouse)' \\\n",
    "--where 'spouse =~ \"^Q[0-9]+\"' \\\n",
    "--return 'count(distinct spouse) as count_spouses_with_qnodes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abigail is disappointed to see that only 449 are q-nodes, so uses grep to see what else is in the file. She sees that the DBpedia data is noisy as there are empty strings, numbers that look like dates and entities that do not correspond to Wikidata entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep -v '\\tQ[0-9]' \"$OUT\"/spouses.ulan.dbpedia.tsv | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abigail does not want to take on what looks like a difficult data cleaning and entity linking job, so she keeps the 449 clean entities and puts them in `spouses.ulan.dbpedia.qnodes.tsv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep '\\tQ[0-9]' \"$OUT\"/spouses.ulan.dbpedia.tsv > \"$OUT\"/spouses.ulan.dbpedia.qnodes.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abigail wants to see how many artists have spouse statements in both Wikidata and DBpedia, so she adds another clause to the query and sees that only 359 are in both, so she can get about 100 new statements from DBpedia. While not a lot, she will diff the two files and keep the new statements (not shown in this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-07-24 22:33:33 query]: SQL Translation:\n",
      "---------------------------------------------\n",
      "  SELECT count(DISTINCT graph_11_c3.\"id\")\n",
      "     FROM graph_1 AS graph_1_c4\n",
      "     INNER JOIN graph_11 AS graph_11_c3, graph_5 AS graph_5_c5, graph_7 AS graph_7_c2, graph_8 AS graph_8_c1\n",
      "     ON graph_11_c3.\"node2\" = graph_1_c4.\"node2\"\n",
      "        AND graph_11_c3.\"node2\" = graph_5_c5.\"node1\"\n",
      "        AND graph_7_c2.\"node1\" = graph_11_c3.\"node1\"\n",
      "        AND graph_7_c2.\"node1\" = graph_1_c4.\"node1\"\n",
      "        AND graph_8_c1.\"node1\" = graph_7_c2.\"node2\"\n",
      "        AND graph_11_c3.\"label\" = ?\n",
      "        AND graph_1_c4.\"label\" = ?\n",
      "        AND graph_7_c2.\"label\" = ?\n",
      "  PARAS: ['property:spouse', 'P26', 'P245']\n",
      "---------------------------------------------\n",
      "[2021-07-24 22:33:33 sqlstore]: CREATE INDEX on table graph_7 column node2 ...\n",
      "[2021-07-24 22:37:57 sqlstore]: ANALYZE INDEX on table graph_7 column node2 ...\n",
      "[2021-07-24 22:38:10 sqlstore]: CREATE INDEX on table graph_7 column label ...\n",
      "[2021-07-24 22:40:53 sqlstore]: ANALYZE INDEX on table graph_7 column label ...\n",
      "[2021-07-24 22:41:04 sqlstore]: CREATE INDEX on table graph_7 column node1 ...\n",
      "[2021-07-24 22:42:57 sqlstore]: ANALYZE INDEX on table graph_7 column node1 ...\n",
      "[2021-07-24 22:43:09 sqlstore]: CREATE INDEX on table graph_11 column node1 ...\n",
      "[2021-07-24 22:44:25 sqlstore]: ANALYZE INDEX on table graph_11 column node1 ...\n",
      "[2021-07-24 22:44:32 sqlstore]: CREATE INDEX on table graph_8 column node1 ...\n",
      "[2021-07-24 22:44:32 sqlstore]: ANALYZE INDEX on table graph_8 column node1 ...\n",
      "count(DISTINCT graph_11_c3.\"id\")\n",
      "359\n",
      "      686.09 real       442.91 user        81.10 sys\n"
     ]
    }
   ],
   "source": [
    "!$kypher -i items -i infobox -i ulan -i external_ids -i labels \\\n",
    "--match '\\\n",
    "    ulan: (ulan_id)-[]->(), \\\n",
    "    external_ids: (artist)-[:P245]->(ulan_id), \\\n",
    "    infobox: (artist)-[l:`property:spouse`]->(spouse), \\\n",
    "    items: (artist)-[:P26]->(spouse), \\\n",
    "    labels: (spouse)-[]->(spouse_label)' \\\n",
    "--return 'count(distinct l)' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spouse statements in DBpedia\n",
    "Abigail gets curious wether it is worth working on a project to augment Wikidata with spouse statements from DBpedia. Now she knows that she needs to focus on the ones that are mapped to q-nodes, so writes a query to fetch all the spouse statements from DBpedia. She wants to make sure to get clean data, so she adds a constraint to verify that the q-nodes she gets from DBpedia are instances of `Q5`  (human) in Wikidata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-07-24 21:21:06 query]: SQL Translation:\n",
      "---------------------------------------------\n",
      "  SELECT graph_11_c1.\"node1\" \"_aLias.node1\", ? \"_aLias.label\", graph_11_c1.\"node2\" \"_aLias.node2\"\n",
      "     FROM graph_11 AS graph_11_c1\n",
      "     INNER JOIN graph_3 AS graph_3_c2\n",
      "     ON graph_11_c1.\"node2\" = graph_3_c2.\"node1\"\n",
      "        AND graph_11_c1.\"label\" = ?\n",
      "        AND graph_3_c2.\"node2\" = ?\n",
      "  PARAS: ['P26', 'property:spouse', 'Q5']\n",
      "---------------------------------------------\n",
      "[2021-07-24 21:21:06 sqlstore]: CREATE INDEX on table graph_11 column label ...\n",
      "[2021-07-24 21:23:08 sqlstore]: ANALYZE INDEX on table graph_11 column label ...\n",
      "[2021-07-24 21:23:15 sqlstore]: CREATE INDEX on table graph_11 column node2 ...\n",
      "[2021-07-24 21:25:15 sqlstore]: ANALYZE INDEX on table graph_11 column node2 ...\n",
      "      323.33 real       219.21 user        41.19 sys\n"
     ]
    }
   ],
   "source": [
    "# dbpedia spouses\n",
    "!$kypher -i infobox -i p31 \\\n",
    "--match ' \\\n",
    "    infobox: (artist)-[:`property:spouse`]->(spouse), \\\n",
    "    p31: (spouse)-[]->(:Q5)' \\\n",
    "--return 'artist as node1, \"P26\" as label, spouse as node2' \\\n",
    "-o \"$OUT\"/spouses.dbpedia.qnodes.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   39778 /Users/pedroszekely/Downloads/kypher/wd-workshop/spouses.dbpedia.qnodes.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc -l \"$OUT\"/spouses.dbpedia.qnodes.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abigail also gets all the spouse statements from Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-07-24 21:30:04 query]: SQL Translation:\n",
      "---------------------------------------------\n",
      "  SELECT graph_1_c1.\"node1\" \"_aLias.node1\", graph_1_c1.\"label\" \"_aLias.label\", graph_1_c1.\"node2\" \"_aLias.node2\"\n",
      "     FROM graph_1 AS graph_1_c1\n",
      "     WHERE graph_1_c1.\"label\" = ?\n",
      "  PARAS: ['P26']\n",
      "---------------------------------------------\n",
      "       47.16 real         3.92 user         6.35 sys\n"
     ]
    }
   ],
   "source": [
    "!$kypher -i items \\\n",
    "--match '(artist)-[l:P26]->(spouse)' \\\n",
    "--return 'artist as node1, l.label as label, spouse as node2' \\\n",
    "-o \"$OUT\"/spouses.wikidata.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  681174 /Users/pedroszekely/Downloads/kypher/wd-workshop/spouses.wikidata.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc -l \"$OUT\"/spouses.wikidata.tsv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abigail sees that there is a significant difference, so writes a query to identify the common statements in both datasets and sees that there are about clean 7,000 spouse statements in DBpedia that are not present in Wikidata. It will be easy to add them using Wikidata quick statements, a project that she will try to do later.\n",
    "> Kypher can run queries over the two new graphs created in the previous queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-07-24 21:30:51 sqlstore]: IMPORT graph directly into table graph_12 from /Users/pedroszekely/Downloads/kypher/wd-workshop/spouses.wikidata.tsv ...\n",
      "[2021-07-24 21:30:53 sqlstore]: IMPORT graph directly into table graph_13 from /Users/pedroszekely/Downloads/kypher/wd-workshop/spouses.dbpedia.qnodes.tsv ...\n",
      "[2021-07-24 21:30:53 query]: SQL Translation:\n",
      "---------------------------------------------\n",
      "  SELECT DISTINCT graph_12_c1.\"node1\" \"_aLias.person\", graph_12_c1.\"node2\" \"_aLias.spouse\"\n",
      "     FROM graph_12 AS graph_12_c1\n",
      "     INNER JOIN graph_13 AS graph_13_c2\n",
      "     ON graph_12_c1.\"node1\" = graph_13_c2.\"node1\"\n",
      "        AND graph_12_c1.\"node2\" = graph_13_c2.\"node2\"\n",
      "  PARAS: []\n",
      "---------------------------------------------\n",
      "[2021-07-24 21:30:53 sqlstore]: CREATE INDEX on table graph_12 column node2 ...\n",
      "[2021-07-24 21:30:53 sqlstore]: ANALYZE INDEX on table graph_12 column node2 ...\n",
      "[2021-07-24 21:30:53 sqlstore]: CREATE INDEX on table graph_13 column node1 ...\n",
      "[2021-07-24 21:30:53 sqlstore]: ANALYZE INDEX on table graph_13 column node1 ...\n",
      "[2021-07-24 21:30:53 sqlstore]: CREATE INDEX on table graph_12 column node1 ...\n",
      "[2021-07-24 21:30:53 sqlstore]: ANALYZE INDEX on table graph_12 column node1 ...\n",
      "[2021-07-24 21:30:54 sqlstore]: CREATE INDEX on table graph_13 column node2 ...\n",
      "[2021-07-24 21:30:54 sqlstore]: ANALYZE INDEX on table graph_13 column node2 ...\n",
      "        3.42 real         4.03 user         0.37 sys\n",
      "   32453\n"
     ]
    }
   ],
   "source": [
    "!$kypher -i \"$OUT\"/spouses.wikidata.tsv  -i \"$OUT\"/spouses.dbpedia.qnodes.tsv \\\n",
    "--match '\\\n",
    "    wikidata: (person)-[]->(spouse), \\\n",
    "    dbpedia: (person)-[]->(spouse)' \\\n",
    "--return 'distinct person as person, spouse as spouse' \\\n",
    "| wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate property constraints in Wikidata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amir and Sarah are starting a project to find constraint violations in Wikidata. They find that constraints are associated with properties using the `P2302` (property constraint) property. There are over 44,000 constraints, so finding violations is a dauting task as many constraints apply to a very large number of statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-07-24 21:44:29 query]: SQL Translation:\n",
      "---------------------------------------------\n",
      "  SELECT count(DISTINCT graph_1_c1.\"id\") \"_aLias.count\"\n",
      "     FROM graph_1 AS graph_1_c1\n",
      "     WHERE graph_1_c1.\"label\" = ?\n",
      "  PARAS: ['P2302']\n",
      "---------------------------------------------\n",
      "count\n",
      "44552\n",
      "        1.43 real         0.89 user         0.25 sys\n"
     ]
    }
   ],
   "source": [
    "!$kypher -i items \\\n",
    "--match '(property)-[l:P2302]->(constraint)' \\\n",
    "--return 'count(distinct l) as count' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarah refines the query to print the counts of the different types of constraints, and they see that 9 constraint types are significantly more popular than the others. Amir and Sarah decide to focus on the `value type constraint` as this is the constraint that checks that the value of a statement belongs to specific classes. This constraint is defined for 964 properties, so it is worth working on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-07-24 21:43:25 query]: SQL Translation:\n",
      "---------------------------------------------\n",
      "  SELECT graph_5_c2.\"node2\" \"_aLias.constraint_label\", count(DISTINCT graph_1_c1.\"id\") \"_aLias.count\"\n",
      "     FROM graph_1 AS graph_1_c1\n",
      "     INNER JOIN graph_5 AS graph_5_c2\n",
      "     ON graph_1_c1.\"node2\" = graph_5_c2.\"node1\"\n",
      "        AND graph_1_c1.\"label\" = ?\n",
      "     GROUP BY \"_aLias.constraint_label\"\n",
      "     ORDER BY \"_aLias.count\" DESC\n",
      "  PARAS: ['P2302']\n",
      "---------------------------------------------\n",
      "        1.34 real         0.91 user         0.23 sys\n",
      "constraint_label                                        count\n",
      "'item requires statement constraint'@en                 7576\n",
      "'allowed entity types constraint'@en                    6401\n",
      "'format constraint'@en                                  6042\n",
      "'distinct values constraint'@en                         5513\n",
      "'single value constraint'@en                            5453\n",
      "'type constraint'@en                                    5070\n",
      "'property scope constraint'@en                          2908\n",
      "'conflicts-with constraint'@en                          1275\n",
      "'value type constraint'@en                              964\n",
      "'allowed qualifiers constraint'@en                      573\n",
      "'allowed units constraint'@en                           483\n",
      "'required qualifier constraint'@en                      391\n",
      "'range constraint'@en                                   327\n",
      "'value requires statement constraint'@en                320\n",
      "'citation needed constraint'@en                         284\n",
      "'one-of constraint'@en                                  156\n",
      "'integer constraint'@en                                 145\n",
      "'contemporary constraint'@en                            124\n",
      "'inverse constraint'@en                                 110\n",
      "'single-best-value constraint'@en                       101\n",
      "'none of constraint'@en                                 74\n",
      "'no bounds constraint'@en                               74\n",
      "'Commons link constraint'@en                            73\n",
      "'symmetric constraint'@en                               44\n",
      "'multi-value constraint'@en                             27\n",
      "'lexeme requires language constraint'@en                25\n",
      "'difference within range constraint'@en                 9\n",
      "'lexeme requires lexical category constraint'@en        5\n",
      "'one-of qualifier value property constraint'@en         4\n",
      "'lexeme value requires lexical category constraint'@en  1\n"
     ]
    }
   ],
   "source": [
    "!$kypher -i items -i labels -i p279star \\\n",
    "--match ' \\\n",
    "    items: (property)-[l:P2302]->(constraint), \\\n",
    "    labels: (constraint)-[]->(constraint_label)' \\\n",
    "--return 'constraint_label as constraint_label, count(distinct l) as count' \\\n",
    "--order-by 'count desc' \\\n",
    "| column -ts $'\\t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Cache:\n",
      "DB file: /Users/pedroszekely/Downloads/kypher/temp.novartis/wikidata.sqlite3.db\n",
      "  size:  140.60 GB   \tfree:  0 Bytes   \tmodified:  2021-07-24 22:44:32\n",
      "\n",
      "KGTK File Information:\n",
      "/Users/pedroszekely/Downloads/kypher/temp.wd-workshop/human.count.year.tsv:\n",
      "  size:  50.79 KB   \tmodified:  2021-07-24 20:51:20   \tgraph:  graph_10\n",
      "/Users/pedroszekely/Downloads/kypher/wd-workshop/given-names.year.tsv:\n",
      "  size:  40.45 MB   \tmodified:  2021-07-24 20:49:54   \tgraph:  graph_9\n",
      "/Users/pedroszekely/Downloads/kypher/wd-workshop/spouses.dbpedia.qnodes.tsv:\n",
      "  size:  826.70 KB   \tmodified:  2021-07-24 21:26:27   \tgraph:  graph_13\n",
      "/Users/pedroszekely/Downloads/kypher/wd-workshop/spouses.wikidata.tsv:\n",
      "  size:  15.26 MB   \tmodified:  2021-07-24 21:30:49   \tgraph:  graph_12\n",
      "external_ids:\n",
      "  size:  3.59 GB   \tmodified:  2021-03-11 06:19:15   \tgraph:  graph_7\n",
      "infobox:\n",
      "  size:  1.46 GB   \tmodified:  2021-07-24 11:42:08   \tgraph:  graph_11\n",
      "items:\n",
      "  size:  7.47 GB   \tmodified:  2021-02-26 08:03:09   \tgraph:  graph_1\n",
      "labels:\n",
      "  size:  2.09 GB   \tmodified:  2021-03-15 17:22:24   \tgraph:  graph_5\n",
      "p279:\n",
      "  size:  37.88 MB   \tmodified:  2021-03-10 20:10:45   \tgraph:  graph_4\n",
      "p279star:\n",
      "  size:  529.33 MB   \tmodified:  2021-03-12 01:04:52   \tgraph:  graph_6\n",
      "p31:\n",
      "  size:  1.09 GB   \tmodified:  2021-04-18 13:13:37   \tgraph:  graph_3\n",
      "time:\n",
      "  size:  809.00 MB   \tmodified:  2021-02-26 04:49:21   \tgraph:  graph_2\n",
      "ulan:\n",
      "  size:  348.04 KB   \tmodified:  2021-07-18 09:31:25   \tgraph:  graph_8\n",
      "\n",
      "Graph Table Information:\n",
      "graph_1:\n",
      "  size:  69.23 GB   \tcreated:  2021-07-24 19:12:24\n",
      "  header:  ['id', 'node1', 'label', 'node2', 'rank', 'node2;wikidatatype']\n",
      "graph_10:\n",
      "  size:  96.00 KB   \tcreated:  2021-07-24 20:51:28\n",
      "  header:  ['node1', 'node2', 'label']\n",
      "graph_11:\n",
      "  size:  16.15 GB   \tcreated:  2021-07-24 21:08:43\n",
      "  header:  ['node1', 'label', 'node2', 'id']\n",
      "graph_12:\n",
      "  size:  43.06 MB   \tcreated:  2021-07-24 21:30:53\n",
      "  header:  ['node1', 'label', 'node2']\n",
      "graph_13:\n",
      "  size:  2.28 MB   \tcreated:  2021-07-24 21:30:53\n",
      "  header:  ['node1', 'label', 'node2']\n",
      "graph_2:\n",
      "  size:  6.13 GB   \tcreated:  2021-07-24 19:18:12\n",
      "  header:  ['id', 'node1', 'label', 'node2', 'rank', 'node2;wikidatatype']\n",
      "graph_3:\n",
      "  size:  8.80 GB   \tcreated:  2021-07-24 19:24:50\n",
      "  header:  ['id', 'node1', 'label', 'node2']\n",
      "graph_4:\n",
      "  size:  191.14 MB   \tcreated:  2021-07-24 19:25:19\n",
      "  header:  ['id', 'node1', 'label', 'node2']\n",
      "graph_5:\n",
      "  size:  9.03 GB   \tcreated:  2021-07-24 19:34:51\n",
      "  header:  ['id', 'node1', 'label', 'node2']\n",
      "graph_6:\n",
      "  size:  7.22 GB   \tcreated:  2021-07-24 19:41:34\n",
      "  header:  ['node1', 'label', 'node2', 'id']\n",
      "graph_7:\n",
      "  size:  23.77 GB   \tcreated:  2021-07-24 20:00:47\n",
      "  header:  ['id', 'node1', 'label', 'node2', 'rank', 'node2;wikidatatype']\n",
      "graph_8:\n",
      "  size:  1016.00 KB   \tcreated:  2021-07-24 20:00:47\n",
      "  header:  ['node1']\n",
      "graph_9:\n",
      "  size:  46.83 MB   \tcreated:  2021-07-24 20:51:28\n",
      "  header:  ['node1', 'year', 'node2', 'node1;label', 'label']\n",
      "        1.22 real         0.81 user         0.20 sys\n"
     ]
    }
   ],
   "source": [
    "!$kypher --show-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Took {:.2f} minutes to run the notebook from start to end\".format((round(time.time()) - start_time)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgtk-env",
   "language": "python",
   "name": "kgtk-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
